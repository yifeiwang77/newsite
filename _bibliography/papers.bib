

@inproceedings{wang2024theoretical,
  title={A Theoretical Understanding of Self-Correction through In-context Alignment},
  author={{<u>Yifei Wang</u>}* and Yuyang Wu* and Zeming Wei and Stefanie Jegelka and Yisen Wang},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18634},
  abbr={NeurIPS},
  code={https://github.com/yifeiwang77/Self-Correction},
  selected = {true},
  note = {
  <span style="font-weight: bold;">Best Paper Award</span>  at ICML 2024 ICL Workshop<br>
  We introduced the first theoretical explanation of how self-correction works in LLMs (as in o1) and showed its effectiveness against social bias and jailbreak attacks.}
}

@inproceedings{wang2024equivariance,
  title={Understanding the Role of Equivariance in Self-supervised Learning},
  author={{<u>Yifei Wang</u>}* and Kaiwen Hu* and Sharut Gupta and Ziyu Ye and Yisen Wang and Stefanie Jegelka},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://openreview.net/pdf?id=yLpuruMZHE},
  code={https://github.com/kaotty/Understanding-ESSL},
  abbr={NeurIPS},
}

@inproceedings{wang2024symmetries,
  title={In-Context Symmetries: Self-Supervised Learning through Contextual World Models},
  author={Sharut Gupta* and Chenyu Wang* and {<u>Yifei Wang</u>}* and Tommi Jaakkola and Stefanie Jegelka},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18193},
  code={https://github.com/Sharut/In-Context-Symmetries},
  abbr={NeurIPS},
  note={<span style="font-weight: bold;">Oral Presentation (top 4)</span>  at NeurIPS 2024 SSL Workshop},
  selected = {true},
  note = {We introduced in-context learning abilities to joint embedding methods, making them more general-purpose and efficiently adaptable to downstream tasks.},
}

@inproceedings{wang2024canonization,
  title={A Canonization Perspective on Invariant and Equivariant Learning},
  author={George Ma* and {<u>Yifei Wang</u>}* and Derek Lim and Stefanie Jegelka and Yisen Wang},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18378},
  code={https://github.com/PKU-ML/canonicalization},
  abbr={NeurIPS},
}

@inproceedings{wang2024attention,
  title={On the Role of Attention Masks and LayerNorm in Transformers},
  author={Xinyi Wu and Amir Ajorlou and {<u>Yifei Wang</u>} and Stefanie Jegelka and Ali Jadbabaie},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18781},
  abbr={NeurIPS},
}

@inproceedings{wang2024dissecting,
  title={Dissecting the Failure of Invariant Learning on Graphs},
  author={Qixun Wang and {<u>Yifei Wang</u>} and Yisen Wang and Xianghua Ying},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://openreview.net/pdf?id=7eFS8aZHAM},
  code={https://github.com/NOVAglow646/NeurIPS24-Invariant-Learning-on-Graphs},
  abbr={NeurIPS},
}

@inproceedings{ye2024reasoning,
  author = {Ziyu Ye and Jiacheng Chen and Jonathan Light and {<u>Yifei Wang</u>} and Jiankai Sun and Mac Schwager and Philip Torr and Guohao Li and Yuxin Chen and Kaiyu Yang and Yisong Yue and Ziniu Hu},
  title = {Reasoning in Reasoning: A Hierarchical Framework for Better and Faster Neural Theorem Proving},
  booktitle = { NeurIPS 2024 Workshop on Mathematical Reasoning and AI},
  pdf = {https://openreview.net/pdf?id=H5hePMXKht},
  year = {2024},
  abbr = {NeurIPS Workshop},
}
@inproceedings{yan2024multifaceted,
  author = {Hanqi Yan and Yulan He and {<u>Yifei Wang</u>} (Corresponding Author)},
  title = {The Multi-faceted Monosemanticity in Multimodal Representations},
  booktitle = {NeurIPS 2024 Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models},
  year = {2024},
  pdf = {https://openreview.net/pdf?id=9NLRpwfLnT},
  abbr = {NeurIPS Workshop},
}

@inproceedings{yan2024monosemanticity,
  title={Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective},
  author={Hanqi Yan and Yanzheng Xiang and Guangyi Chen and {<u>Yifei Wang</u>} and Lin Gui and Yulan He},
  booktitle={EMNLP},
  year={2024},
  pdf={https://arxiv.org/pdf/2406.17969v1},
  code={https://github.com/hanqi-qi/revisit_monosemanticity},
  abbr={EMNLP},
}

@inproceedings{zhang2024look,
  title={Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining},
  author={Qi Zhang and Tianqi Du and Haotian Huang and {<u>Yifei Wang</u>} and Yisen Wang},
  booktitle={ICML},
  year={2024},
  abbr={ICML},
  pdf={https://openreview.net/pdf?id=2rPoTgEmjV},
  code={https://github.com/PKU-ML/LookAheadLookAround},
}

@inproceedings{li2024oodrobustbench,
  title={OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift},
  author={Lin Li and {<u>Yifei Wang</u>} and Chawin Sitawarin and Michael W. Spratling},
  booktitle={ICML},
  year={2024},
  pdf={https://arxiv.org/pdf/2310.12793},
  code={https://github.com/OODRobustBench/OODRobustBench},
  abbr={ICML},
}

@inproceedings{zhang2024sharpness,
  title={On the Duality Between Sharpness-Aware Minimization and Adversarial Training},
  author={Yihao Zhang and Hangzhou He and Jingyu Zhu and Huanran Chen and {<u>Yifei Wang</u>} and Zeming Wei},
  booktitle={ICML},
  year={2024},
  pdf={https://arxiv.org/abs/2402.15152},
  code={https://github.com/weizeming/SAM_AT},
  abbr={ICML},
}

@inproceedings{fang2024rethinking,
  title={Rethinking Invariance in In-context Learning},
  author={Lizhe Fang* and {<u>Yifei Wang</u>}* and Khashayar Gatmiry and Lei Fang and Yisen Wang},
  booktitle={ICML Workshop on Theoretical Foundations of Foundation Models (TF2M)},
  year={2024},
  pdf={https://openreview.net/pdf?id=xSDqxxILWg},
  abbr={ICML},
}

@inproceedings{wang2024nonnegative,
  title={Non-negative Contrastive Learning},
  author={{<u>Yifei Wang</u>}* and Qi Zhang* and Yaoyu Guo and Yisen Wang},
  booktitle={ICLR},
  year={2024},
  pdf={https://arxiv.org/pdf/2403.12459},
  code={https://github.com/PKU-ML/non_neg},
  slides={NCL_LIDS_tea.pdf},
  abbr={ICLR},
  selected = {true},
  note = {Drawing inspirations from Non-negative Matrix Factorization (NMF), we introduced a principled one-line technique that significantly boosts representation interpretability.}
}

@inproceedings{wang2024generated,
  title={Do Generated Data Always Help Contrastive Learning?},
  author={{<u>Yifei Wang</u>}* and Jizhe Zhang* and Yisen Wang},
  booktitle={ICLR},
  year={2024},
  pdf={https://arxiv.org/pdf/2403.12448.pdf},
  code={https://github.com/PKU-ML/adainf},
  featured={https://mp.weixin.qq.com/s/MSSzIl3KnvRzgWVN0ZyW6A},
  abbr={ICLR},
  selected = {true},
  note = {We revealed both theoretically and practically that synthetic data introduces fundamental bias to SSL generalization, but, with an adaptive strategy of data mixing and augmentation, can yield substantial benefits.}
}

@inproceedings{du2024discrete,
  title={On the Role of Discrete Tokenization in Visual Representation Learning},
  author={Tianqi Du* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={ICLR <span style="font-weight: bold;">Spotlight</span>},
  year={2024},
  pdf={https://openreview.net/pdf?id=WNLAkjUm19},
  code={https://github.com/PKU-ML/ClusterMIM},
  abbr={ICLR Spotlight},
}

@inproceedings{wang2023balance,
  title={Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective},
  author={{<u>Yifei Wang</u>}* and Liangchen Li* and Jiansheng Yang and Zhouchen Lin and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.19360.pdf},
  code={https://github.com/PKU-ML/ReBAT},
  abbr={NeurIPS},
}

@inproceedings{li2023adversarial,
  title={Adversarial Examples Are Not Real Features},
  author={Ang Li* and {<u>Yifei Wang</u>}* and Yiwen Guo and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.18936.pdf},
  code={https://github.com/PKU-ML/AdvNotRealFeatures},
  abbr={NeurIPS},
}

@inproceedings{guo2023architecture,
  title={Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning},
  author={Xiaojun Guo* and {<u>Yifei Wang</u>}* and Zeming Wei and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2311.02687.pdf},
  code={https://github.com/PKU-ML/ArchitectureMattersGCL},
  abbr={NeurIPS},
}

@inproceedings{zhang2023contrastive,
  title={Identifiable Contrastive Learning with Automatic Feature Importance Discovery},
  author={Qi Zhang* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.18904.pdf},
  code={https://github.com/PKU-ML/Tri-factor-Contrastive-Learning},
  abbr={NeurIPS},
}

@inproceedings{ma2023laplacian,
  title={Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding},
  author={George Ma* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.18716.pdf},
  code={https://github.com/PKU-ML/LaplacianCanonization},
  abbr={NeurIPS},
}

@inproceedings{zhang2023generalization,
  title={On the Generalization of Multi-modal Contrastive Learning},
  author={Qi Zhang* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={ICML},
  year={2023},
  pdf={https://arxiv.org/pdf/2306.04272},
  code={https://github.com/PKU-ML/CLIP-Help-SimCLR},
  abbr={ICML},
  selected = {true},
  note={We established the first generalization analysis for multi-modal contrastive learning (e.g., CLIP) and explained how it outperforms self-supervised contrastive learning.}
}

@inproceedings{cui2023rethinking,
  title={Rethinking Weak Supervision in Helping Contrastive Representation Learning},
  author={Jingyi Cui* and Weiran Huang* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={ICML},
  year={2023},
  pdf={https://arxiv.org/pdf/2306.04160},
  abbr={ICML},
}

@inproceedings{wei2023cfa,
  title={CFA: Class-wise Calibrated Fair Adversarial Training},
  author={Zeming Wei and {<u>Yifei Wang</u>} and Yiwen Guo and Yisen Wang},
  booktitle={CVPR},
  year={2023},
  pdf={https://arxiv.org/pdf/2303.14460.pdf},
  code={https://github.com/PKU-ML/CFA},
  abbr={CVPR},
}

@article{chen2023equilibrium,
  title={Equilibrium Image Denoising with Implicit Differentiation},
  author={Qi Chen and {<u>Yifei Wang</u>} and Zhengyang Geng and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  journal={IEEE Transactions on Image Processing (TIP)},
  year={2023},
  pdf={https://zhouchenlin.github.io/Publications/2023-TIP-Denoising.pdf},
  abbr={TIP},
}

@inproceedings{wang2023message,
  title={A Message Passing Perspective on Learning Dynamics of Contrastive Learning},
  author={{<u>Yifei Wang</u>}* and Qi Zhang* and Tianqi Du and Jiansheng Yang and Zhouchen Lin and Yisen Wang},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=VBTJqqWjxMv},
  code={https://github.com/PKU-ML/Message-Passing-Contrastive-Learning},
  slides={ICLR23_Message_Passing.pdf},
  blog={https://mp.weixin.qq.com/s/e18pZfee7ffwAHMUBZ_OEg},
  abbr={ICLR},
  selected = {true},
  note = {We revealed that contrastive learning performs message passing on sample graph, which connects self-supervised learning and graph neural networks as a whole.}
}

@inproceedings{zhuo2023noncontrastive,
  title={Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism},
  author={Zhijian Zhuo* and {<u>Yifei Wang</u>}* and Jinwen Ma and Yisen Wang},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=cIbjyd2Vcy},
  code={https://github.com/PKU-ML/Rank-Differential-Mechanism},
  abbr={ICLR},
  selected = {true},
  note = {We revealed that various asymmtric designs in non-contrastive learning (BYOL, SimSiam, DINO, SwAV) can be explained from a unified spectral filtering perspective.}
}

@inproceedings{luo2023augmentation,
  title={Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning},
  author={Rundong Luo* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=0qmwFNJyxCL},
  code={https://github.com/PKU-ML/DynACL},
  abbr={ICLR},
}

@inproceedings{guo2023contranorm,
  title={ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond},
  author={Xiaojun Guo* and {<u>Yifei Wang</u>}* and Tianqi Du* and Yisen Wang},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=SM7XkJouWHm},
  code={https://github.com/PKU-ML/ContraNorm},
  abb={ICLR},
}

@inproceedings{li2023solver,
  title={Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States},
  author={Mingjie Li and {<u>Yifei Wang</u>} and Yisen Wang and Zhouchen Lin},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=j3cUWIMsFBN},
  abbr={ICLR},
}


@inproceedings{xin2023invariant,
  title={On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization},
  author={Shiji Xin and {<u>Yifei Wang</u>} and Jingtong Su and Yisen Wang},
  booktitle={AAAI},
  year={2023},
  note={<span style="font-weight: bold;">Oral</span>},
  pdf={https://arxiv.org/pdf/2212.09082.pdf},
  abbr={AAAI},
}

@inproceedings{zhang2022mae,
  title={How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders},
  author={Qi Zhang* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)},
  year={2022},
  pdf={https://arxiv.org/pdf/2210.08344},
  code={https://github.com/zhangq327/U-MAE},
  slides={NeurIPS2022_mae.pdf},
  abbr={NeurIPS Spotlight},
  selected = {true},
  note = {We established the first generalization analysis of masked autoencoders and revealed an inherent connection to contrastive learning.}
}

@inproceedings{wang2022ood,
  title={Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors},
  author={Qixun Wang* and {<u>Yifei Wang</u>}* and Hong Zhu and Yisen Wang},
  booktitle={NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)},
  year={2022},
  pdf={https://arxiv.org/pdf/2210.06807},
  code={https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD},
  slides={NeurIPS2022_OOD.pdf},
  abbr={NeurIPS Spotlight},
}

@inproceedings{mo2022vision,
  title={When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture},
  author={Yichuan Mo and Dongxian Wu and {<u>Yifei Wang</u>} and Yiwen Guo and Yisen Wang},
  booktitle={NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)},
  year={2022},
  pdf={https://arxiv.org/pdf/2210.07540.pdf},
  code={https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers},
  abbr={NeurIPS Spotlight},
}


@inproceedings{cui2022aggnce,
  title={AggNCE: Asymptotically Identifiable Contrastive Learning},
  author={Jingyi Cui* and Weiran Huang* and {<u>Yifei Wang</u>} and Yisen Wang},
  booktitle={NeurIPS SSL Workshop},
  year={2022},
  note={Oral},
  pdf={https://sslneurips22.github.io/paper_pdfs/paper_68.pdf},
  abbr={NeurIPS Workshop},
}

@inproceedings{chen2022diffusion,
  title={Optimization-Induced Graph Implicit Nonlinear Diffusion},
  author={Qi Chen and {<u>Yifei Wang</u>} and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={ICML},
  year={2022},
  pdf={https://proceedings.mlr.press/v162/chen22z/chen22z.pdf},
  code={https://github.com/7qchen/GIND},
  abbr={ICML},
}

@inproceedings{li2022g2cn,
  title={G2CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters},
  author={Mingjie Li and Xiaojun Guo and {<u>Yifei Wang</u>} and Yisen Wang and Zhouchen Lin},
  booktitle={ICML},
  year={2022},
  pdf={https://proceedings.mlr.press/v162/li22h/li22h.pdf},
  abbr={ICML},
}

@inproceedings{wang2022chaos,
  title={Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap},
  author={{<u>Yifei Wang</u>}* and Qi Zhang* and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={ICLR},
  year={2022},
  pdf={http://arxiv.org/pdf/2203.13457},
  code={https://github.com/zhangq327/ARC},
  slides={ICLR2022_overlap.pdf},
  abbr={ICLR},
  selected = {true},
  note = {We established a new graph perspective to formulate how contrastive learning works, and established practical generalization bounds and unsupervised measures.}
}

@inproceedings{wang2022cem,
  title={A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training},
  author={{<u>Yifei Wang</u>} and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={ICLR},
  year={2022},
  pdf={http://arxiv.org/pdf/2203.13455},
  slides={ICLR2022_CEM.pdf},
  award={https://advml-workshop.github.io/icml2021/},
  abbr={ICLR},
  selected = {true},
  note = {
  <span style="font-weight: bold;">Silver Best Paper Award</span> at ICML 2021 AdvML workshop<br>
  From an energy-based perspective, we formulated contrastive learning as a generative model, and  established the connection between adversarial training and maximum likelihood, thus briding generative and discriminative models together.}
}

@inproceedings{wang2021residual,
  title={Residual Relaxation for Multi-view Representation Learning},
  author={{<u>Yifei Wang</u>} and Zhengyang Geng and Feng Jiang and Chuming Li and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={NeurIPS},
  year={2021},
  pdf={https://arxiv.org/pdf/2110.15348},
  slides={NeurIPS2021_Prelax_slides.pdf},
  blog={https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw},
  abbr={NeurIPS},
}

@inproceedings{wang2021dissecting,
  title={Dissecting the Diffusion Process in Linear Graph Convolutional Networks},
  author={{<u>Yifei Wang</u>} and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={NeurIPS},
  year={2021},
  pdf={https://arxiv.org/pdf/2102.10739},
  code={https://github.com/yifeiwang77/DGC},
  slides={NeurIPS2021_DGC_slides.pdf},
  blog={https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA},
  abbr={NeurIPS},
}

@inproceedings{wang2021reparameterized,
  title={Reparameterized Sampling for Generative Adversarial Networks},
  author={{<u>Yifei Wang</u>} and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={ECML-PKDD},
  year={2021},
  pdf={https://arxiv.org/pdf/2107.00352},
  code={https://github.com/yifeiwang77/repgan},
  slides={ECML2021_REPGAN_slides.pdf},
  media={https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ},
  talk={https://www.bilibili.com/video/BV1sL4y167gj},
  award={https://ecmlpkdd.org/2021/},
  abbr={ECML-PKDD},
  selected = {true},
  note = {
  <span style="font-weight: bold;">Best ML Paper Award (1/685)</span>, invited to <i>Machine Learning</i><br>
  We explored using GAN discriminator (as a good reward model) to bootstrap sample quality through an efficient MCMC algorithm, which not only guarantees theoretical  convergence but also improves sample efficiency and quality in practice.}
}
