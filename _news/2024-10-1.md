---
layout: post
date: 2024-10-01
inline: true
related_posts: false
---

6 papers were accepted to NeurIPS 2024. In the foundation model world, we demystified how LLMs are capable of self-correction ([paper](https://arxiv.org/pdf/2405.18634)), how to make wonderful joint embedding models capable of *representation-space in-context learning* ([paper](https://arxiv.org/pdf/2405.18193)), why predicting data corruptions (e.g., noise) learns good representations ([paper](https://openreview.net/pdf?id=NLqdudgBfy)), and how Transformers avoid feature collapse with LayerNorm ([paper](https://arxiv.org/pdf/2405.18781)). 
