---
layout: about
title: about
permalink: /
subtitle: Postdoc at <a href='https://www.csail.mit.edu/'>MIT CSAIL</a> 

profile:
  align: right
  image: avatar.JPG
  image_circular: false # crops the image to make it circular
social: true # includes social icons at the bottom of the page
news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
---

I am a machine learning researcher at MIT CSAIL, advised by [Stefanie Jegelka](https://people.csail.mit.edu/stefje/). My research has contributed to theoretical principles of foundation models (generative and representation models) and efficient algorithms for model capabilities and safety:
- **Mathematical Principles of Foundation Models**. I developed theoretical foundations for a broad spectrum of self-supervised learning (SSL) methods used in pretraining, encompassing contrastive [[1](http://arxiv.org/pdf/2203.13457), [2](https://openreview.net/pdf?id=VBTJqqWjxMv)], non-contrastive [[3](https://openreview.net/pdf?id=cIbjyd2Vcy)], reconstructive [[4](https://arxiv.org/pdf/2210.08344)], autoregressive [[5](https://openreview.net/pdf?id=2rPoTgEmjV)], and predictive [[6](https://openreview.net/pdf?id=yLpuruMZHE), [7](https://arxiv.org/pdf/2102.10739)] approaches. I also analyzed the feature dynamics in deep neural networks [[8](https://openreview.net/pdf?id=7eFS8aZHAM), [9](https://arxiv.org/pdf/2405.18781)] and pioneered a rigorous understanding [[10](https://arxiv.org/pdf/2405.18634)] of LLMsâ€™ self-correction ability, a critical component for test-time reasoning.
- **Improving Model Capabilities**. I leveraged these principles to "debug" foundation models. I addressed the rank collapse issue in neural networks [[11](https://arxiv.org/pdf/2311.02687.pdf), [12](https://proceedings.mlr.press/v162/chen22z/chen22z.pdf)], generalized self-supervised learning with unsupervised world models [[13](https://arxiv.org/pdf/2405.18193)], alleviated data shortage  by adaptively utilizing AI-generated data [[14](https://arxiv.org/pdf/2403.12448.pdf)], and enhanced LLM long-context understanding with novel perplexity measures [[15](https://arxiv.org/pdf/2410.23771)].
- **Trustworthy Foundation Models**. I contributed to theory-inspired algorithms to build trustworthy foundation models with respect to adversarial robustness [[16](http://arxiv.org/pdf/2203.13455), [17](https://arxiv.org/pdf/2210.07540.pdf), [18](https://arxiv.org/pdf/2310.19360.pdf), [19](https://arxiv.org/pdf/2310.18936.pdf)], feature interpretability [[20](https://arxiv.org/pdf/2310.18904.pdf), [21](https://arxiv.org/pdf/2403.12459)], and domain generalization [[22](https://arxiv.org/pdf/2210.06807), [23](https://arxiv.org/pdf/2212.09082.pdf), [24](https://arxiv.org/pdf/2310.12793)]. In particular, I contributed to the use of LLMsâ€™ own emergent abilities (such as in-context learning [[25](https://arxiv.org/pdf/2310.06387)] and self-correction [[10](https://arxiv.org/pdf/2405.18634)]) for jailbreaking and defending LLMs (featured and scaled up by [Anthropic](https://www.anthropic.com/research/many-shot-jailbreaking)).
 
My first-author papers received the Best ML Paper Award at ECML-PKDD 2021, the Silver Best Paper Award at ICML 2021 AdvML workshop, and the Best Paper Award at ICML 2024 ICL workshop. My thesis won the CAAI Outstanding Ph.D. Dissertation Runner-Up Award. I published 33 papers at NeurIPS, ICLR, and ICML, and I am a (co)-first author on 22 of them. 

I served as an area chair for ICLR 2024 and 2025 and as a regular reviewer for main AI/ML conferences (NeurIPS, ICML, ECML, AISTATS, LoG, CVPR, ACL). I co-organized the [NeurIPS 2024 Workshop on Red Teaming GenAI](https://redteaming-gen-ai.github.io/) and the  [MIT ML Tea Seminar](https://projects.csail.mit.edu/ml-tea/).

I obtained my PhD in Applied Mathematics from Peking University in 2023, advised by [Yisen Wang](https://yisenwang.github.io), [Zhouchen Lin](https://zhouchenlin.github.io/), [Jiansheng Yang](https://www.math.pku.edu.cn/jsdw/js_20180628175159671361/y_20180628175159671361/69984.htm). Prior to that, I did my undergraduate at School of Mathematical Sciences, Peking University. More details in my [CV](assets/pdf/CV-Yifei Wang-MIT-Dec2024.pdf) ðŸ“‘.